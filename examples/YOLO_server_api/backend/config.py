from __future__ import annotations

import os
import warnings

# Configuration parameters for YOLO Server API

# Whether to use TensorRT for inference
USE_TENSORRT: bool = os.getenv('USE_TENSORRT', 'false').lower() == 'true'

# Whether to use SAHI for sliced inference (only works with .pt models)
USE_SAHI: bool = os.getenv('USE_SAHI', 'false').lower() == 'true'

# Base model variants
MODEL_VARIANTS_ENV: str = os.getenv(
    'MODEL_VARIANTS', 'yolo26x,yolo26l,yolo26m,yolo26s,yolo26n',
)
MODEL_VARIANTS: list[str] = [
    v.strip() for v in MODEL_VARIANTS_ENV.split(',') if v.strip()
]
if not MODEL_VARIANTS:  # Fallback protection
    MODEL_VARIANTS = ['yolo26n']

# Whether to enable lazy loading of models:
# True means models are loaded only when first used
LAZY_LOAD_MODELS: bool = (
    os.getenv('LAZY_LOAD_MODELS', 'true').lower() == 'true'
)

# Maximum number of models allowed in memory simultaneously in lazy loading
# mode (LRU eviction)
MAX_LOADED_MODELS: int = int(os.getenv('MAX_LOADED_MODELS', '5'))

# Whether to preload the smallest model at startup (only in lazy mode)
PRELOAD_SMALLEST: bool = (
    os.getenv('PRELOAD_SMALLEST', 'true').lower() == 'true'
)

# Whether to explicitly call torch.cuda.empty_cache() when releasing/evicting
# models
EXPLICIT_CUDA_CLEANUP: bool = (
    os.getenv('EXPLICIT_CUDA_CLEANUP', 'true').lower() == 'true'
)

# Configuration validation: SAHI mode enforces .pt file usage and is
# incompatible with TensorRT
if USE_SAHI and USE_TENSORRT:
    warnings.warn(
        'USE_SAHI=True forces .pt model usage, overriding USE_TENSORRT=True',
        UserWarning,
        stacklevel=2,
    )

# Display current configuration
_CONFIG_INFO: str = f"""
ðŸ”§ YOLO Server API Configuration:
   â€¢ USE_TENSORRT: {USE_TENSORRT}
   â€¢ USE_SAHI: {USE_SAHI}
   â€¢ Model file format: {'.pt' if USE_SAHI or not USE_TENSORRT else '.engine'}
   â€¢ Inference method: {
    'SAHI slicing' if USE_SAHI else
    'TensorRT' if USE_TENSORRT else
    'Standard YOLO'
}
   â€¢ Model variants: {', '.join(MODEL_VARIANTS)}
"""

print(_CONFIG_INFO)

# Type hints and docstrings added for clarity and maintainability


def get_model_variants() -> list[str]:
    """Retrieve the list of model variants from the environment variable.

    Returns:
        list[str]: A list of model variant names.
    """
    return MODEL_VARIANTS


def is_lazy_loading_enabled() -> bool:
    """Check if lazy loading of models is enabled.

    Returns:
        bool: True if lazy loading is enabled, False otherwise.
    """
    return LAZY_LOAD_MODELS


def get_max_loaded_models() -> int:
    """Get the maximum number of models allowed in memory simultaneously.

    Returns:
        int: The maximum number of models.
    """
    return MAX_LOADED_MODELS


def should_preload_smallest_model() -> bool:
    """Determine if the smallest model should be preloaded at startup.

    Returns:
        bool: True if the smallest model should be preloaded, False otherwise.
    """
    return PRELOAD_SMALLEST


def should_cleanup_cuda_cache() -> bool:
    """Check if explicit CUDA cache cleanup is enabled.

    Returns:
        bool: True if CUDA cache cleanup is enabled, False otherwise.
    """
    return EXPLICIT_CUDA_CLEANUP
